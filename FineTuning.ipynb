{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from open_lm.hf import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"apple/DCLM-Baseline-7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"apple/DCLM-Baseline-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"Machine learning is\"], return_tensors=\"pt\")\n",
    "gen_kwargs = {\"max_new_tokens\": 50, \"top_p\": 0.8, \"temperature\": 0.8, \"do_sample\": True, \"repetition_penalty\": 1.1}\n",
    "output = model.generate(inputs['input_ids'], **gen_kwargs)\n",
    "output = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare daraset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "\treturn tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "\t    report_to = \"none\",\n",
    "\t    output_dir=\"./results\",\n",
    "\t    evaluation_strategy=\"epoch\",\n",
    "\t    learning_rate=2e-5,  # Controls how much to change the model weights during training\n",
    "\t    per_device_train_batch_size=2,  # Number of samples per batch per device during training\n",
    "\t    per_device_eval_batch_size=2,  # Number of samples per batch per device during evaluation\n",
    "\t    num_train_epochs=3,  # Number of times the entire training dataset will be passed through the model\n",
    "\t    weight_decay=0.01,  # Regularization technique to prevent overfitting\n",
    "\t)\n",
    "trainer = Trainer(\n",
    "\t    model=model,\n",
    "\t    args=training_args,\n",
    "\t    train_dataset=tokenized_datasets['train'],\n",
    "\t    eval_dataset=tokenized_datasets['test'],\n",
    "\t    data_collator=data_collator,\n",
    "\t    tokenizer=tokenizer,\n",
    "\t)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
